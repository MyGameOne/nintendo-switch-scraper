name: Nintendo Switch Scraper

on:
  # æ¯å¤© UTC 0ç‚¹ï¼ˆåŒ—äº¬æ—¶é—´ 8ç‚¹ï¼‰è¿è¡Œ
  schedule:
    - cron: '0 0 * * *'
  
  # å½“ data/game-ids.json æ–‡ä»¶å˜æ›´æ—¶è§¦å‘
  push:
    paths:
      - 'data/game-ids.json'
    branches:
      - main
      - master
  
  # å…è®¸æ‰‹åŠ¨è§¦å‘
  workflow_dispatch:
    inputs:
      concurrent:
        description: 'å¹¶å‘æ•°é‡'
        required: false
        default: '3'
        type: string
      delay_min:
        description: 'æœ€å°å»¶è¿Ÿ(ms)'
        required: false
        default: '2000'
        type: string
      delay_max:
        description: 'æœ€å¤§å»¶è¿Ÿ(ms)'
        required: false
        default: '5000'
        type: string

env:
  # é»˜è®¤é…ç½®
  SCRAPER_CONCURRENT: ${{ github.event.inputs.concurrent || '3' }}
  SCRAPER_DELAY_MIN: ${{ github.event.inputs.delay_min || '2000' }}
  SCRAPER_DELAY_MAX: ${{ github.event.inputs.delay_max || '5000' }}
  SCRAPER_HEADLESS: true
  SCRAPER_PARALLEL: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 8
      
      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV
      
      - name: Setup pnpm cache
        uses: actions/cache@v3
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-
      
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      
      - name: Validate game-ids.json
        run: |
          if [ ! -f "data/game-ids.json" ]; then
            echo "âŒ data/game-ids.json æ–‡ä»¶ä¸å­˜åœ¨"
            exit 1
          fi
          
          # éªŒè¯ JSON æ ¼å¼
          if ! jq empty data/game-ids.json; then
            echo "âŒ game-ids.json æ ¼å¼æ— æ•ˆ"
            exit 1
          fi
          
          # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°ç»„
          if [ "$(jq type data/game-ids.json)" != '"array"' ]; then
            echo "âŒ game-ids.json å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼"
            exit 1
          fi
          
          GAME_COUNT=$(jq length data/game-ids.json)
          echo "âœ… æ‰¾åˆ° game-ids.json æ–‡ä»¶"
          echo "ğŸ“‹ æ¸¸æˆ ID æ•°é‡: $GAME_COUNT"
          
          if [ "$GAME_COUNT" -eq 0 ]; then
            echo "âš ï¸ æ²¡æœ‰æ¸¸æˆ ID éœ€è¦å¤„ç†"
            exit 0
          fi
          
          # éªŒè¯æ¸¸æˆ ID æ ¼å¼ï¼ˆ16ä½åå…­è¿›åˆ¶ï¼‰
          INVALID_IDS=$(jq -r '.[] | select(test("^[0-9a-fA-F]{16}$") | not)' data/game-ids.json)
          if [ -n "$INVALID_IDS" ]; then
            echo "âŒ å‘ç°æ— æ•ˆçš„æ¸¸æˆ ID æ ¼å¼:"
            echo "$INVALID_IDS"
            exit 1
          fi
          
          echo "âœ… æ‰€æœ‰æ¸¸æˆ ID æ ¼å¼éªŒè¯é€šè¿‡"
      
      - name: Install Playwright browsers
        run: pnpm exec playwright install chromium
      
      - name: Run scraper
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CLOUDFLARE_D1_DATABASE_ID: ${{ secrets.CLOUDFLARE_D1_DATABASE_ID }}
        run: |
          echo "ğŸš€ å¼€å§‹è¿è¡Œçˆ¬è™«..."
          echo "âš™ï¸ é…ç½®ä¿¡æ¯:"
          echo "   å¹¶å‘æ•°: $SCRAPER_CONCURRENT"
          echo "   å»¶è¿ŸèŒƒå›´: ${SCRAPER_DELAY_MIN}-${SCRAPER_DELAY_MAX}ms"
          echo "   æ— å¤´æ¨¡å¼: $SCRAPER_HEADLESS"
          echo "   å¹¶è¡Œæ¨¡å¼: $SCRAPER_PARALLEL"
          echo ""
          
          # è¿è¡Œçˆ¬è™«å¹¶æ•è·è¾“å‡º
          pnpm scrape 2>&1 | tee scraper.log
      
      - name: Generate summary
        if: always()
        run: |
          echo "## ğŸ® Nintendo Switch çˆ¬è™«è¿è¡ŒæŠ¥å‘Š" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "scraper.log" ]; then
            SUCCESS_COUNT=$(grep -c "âœ… æˆåŠŸå¤„ç†:" scraper.log || echo "0")
            FAILED_COUNT=$(grep -c "âŒ çˆ¬å–å¤±è´¥:" scraper.log || echo "0")
            TOTAL_COUNT=$((SUCCESS_COUNT + FAILED_COUNT))
            
            echo "### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯" >> $GITHUB_STEP_SUMMARY
            echo "- æ€»è®¡: $TOTAL_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- æˆåŠŸ: $SUCCESS_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- å¤±è´¥: $FAILED_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$FAILED_COUNT" -gt 0 ]; then
              echo "### âŒ å¤±è´¥çš„æ¸¸æˆ ID" >> $GITHUB_STEP_SUMMARY
              grep "âŒ çˆ¬å–å¤±è´¥:" scraper.log | sed 's/.*çˆ¬å–å¤±è´¥: /- /' >> $GITHUB_STEP_SUMMARY || true
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "### âœ… æˆåŠŸå¤„ç†çš„æ¸¸æˆ" >> $GITHUB_STEP_SUMMARY
            grep "âœ… æˆåŠŸå¤„ç†:" scraper.log | sed 's/.*æˆåŠŸå¤„ç†: /- /' | head -10 >> $GITHUB_STEP_SUMMARY || true
            
            if [ "$SUCCESS_COUNT" -gt 10 ]; then
              echo "- ... è¿˜æœ‰ $((SUCCESS_COUNT - 10)) ä¸ªæ¸¸æˆ" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âŒ æœªæ‰¾åˆ°è¿è¡Œæ—¥å¿—" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            scraper.log
            *.log
          retention-days: 30
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "ğŸš¨ çˆ¬è™«è¿è¡Œå¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—ã€‚"
          exit 1